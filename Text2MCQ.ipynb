{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPK7LXEeC4IZ7bBOFMKenx+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Installing the required libraries."],"metadata":{"id":"6SGBPA7DAx5h"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6bePKo-WrB97","executionInfo":{"status":"ok","timestamp":1679822158661,"user_tz":-345,"elapsed":31037,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"49b4e03f-70a3-4076-ef26-0a556d1ed045"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (0.40.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==2.9.0\n","  Downloading transformers-2.9.0-py3-none-any.whl (635 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (2.27.1)\n","Collecting tokenizers==0.7.0\n","  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (4.65.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (3.10.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (2022.10.31)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.9.0) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.9.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.9.0) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==2.9.0) (2.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.9.0) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.9.0) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==2.9.0) (1.1.1)\n","Building wheels for collected packages: tokenizers, sacremoses\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0m  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=c45d1739885bac6fdbce922d2d61634611269e22c0671047ea27ccb60b4114ea\n","  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n","Successfully built sacremoses\n","Failed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting nltk==3.4.5\n","  Downloading nltk-3.4.5.zip (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.4.5) (1.16.0)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449920 sha256=af766687f173eace7970119f603486ebcec77a32b2fa63f06a4e711280652524\n","  Stored in directory: /root/.cache/pip/wheels/04/32/57/69e42ad50941013def31e288c6e06bb569442dd993a123cb76\n","Successfully built nltk\n","Installing collected packages: nltk\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.8.1\n","    Uninstalling nltk-3.8.1:\n","      Successfully uninstalled nltk-3.8.1\n","Successfully installed nltk-3.4.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nltk"]}}},"metadata":{}}],"source":["!pip install wheel\n","!pip install transformers==2.9.0\n","!pip install nltk==3.4.5"]},{"cell_type":"code","source":["!pip install nltk==3.4.5\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OilXf_tPvMzX","executionInfo":{"status":"ok","timestamp":1678707284725,"user_tz":-345,"elapsed":3711,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"a922e9c7-35ae-4be3-baa3-699b782eb8de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.9/dist-packages (3.4.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.4.5) (1.15.0)\n"]}]},{"cell_type":"code","source":["!pip install transformers==2.9.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kCeFjy9KwEZO","executionInfo":{"status":"ok","timestamp":1678707306043,"user_tz":-345,"elapsed":12130,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"0a2ecf67-0c77-4363-c527-4773088d6f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==2.9.0\n","  Using cached transformers-2.9.0-py3-none-any.whl (635 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (3.9.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (2022.6.2)\n","Collecting sentencepiece\n","  Using cached sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","Collecting tokenizers==0.7.0\n","  Using cached tokenizers-0.7.0.tar.gz (81 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","source":["!pip install wheel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idUBWe-BwHvG","executionInfo":{"status":"ok","timestamp":1678707466708,"user_tz":-345,"elapsed":3705,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"68ba60e6-4e4b-458f-b603-874bc5d88f6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (0.38.4)\n"]}]},{"cell_type":"code","source":["!pip install transformers==2.9.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rc5PXq5Fwwyr","executionInfo":{"status":"ok","timestamp":1678707507349,"user_tz":-345,"elapsed":13766,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"473e2d78-f797-4146-f5b3-1a9ea3397205"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==2.9.0\n","  Using cached transformers-2.9.0-py3-none-any.whl (635 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (3.9.0)\n","Collecting sacremoses\n","  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==2.9.0) (2022.6.2)\n","Collecting sentencepiece\n","  Using cached sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","Collecting tokenizers==0.7.0\n","  Using cached tokenizers-0.7.0.tar.gz (81 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","source":["!pip3 install wheel\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqAI3tLtw4VI","executionInfo":{"status":"ok","timestamp":1678707637133,"user_tz":-345,"elapsed":3920,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"68b733d5-6a70-4e39-b39b-66b3c4556fae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (0.38.4)\n"]}]},{"cell_type":"code","source":["!pip install transformers --no-cache-dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ee09haqlxZBr","executionInfo":{"status":"ok","timestamp":1679822198444,"user_tz":-345,"elapsed":15308,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"a8c9d01d-471a-4012-98e1-f270eeacd420"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m160.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yuH8KsAxqTc","executionInfo":{"status":"ok","timestamp":1679822207065,"user_tz":-345,"elapsed":4381,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"47f9d112-0537-48d8-c2f4-8f5f1bbc46d7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","\n","sentence1 = \"I had an amazon delivery 2 days ago\"\n","sentence2 = \"The animals in amazon were amazing\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4cy5FxDymXq","executionInfo":{"status":"ok","timestamp":1679822260812,"user_tz":-345,"elapsed":456,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"1b245758-cd88-4366-83f9-90af55b73528"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["#example\n","original_word = \"amazon\"\n","\n","syns = wn.synsets(original_word, 'n')\n","\n","for syn in syns:\n","  print (syn, \": \",syn.definition(),\"\\n\" )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6kGCBbJzmXq","executionInfo":{"status":"ok","timestamp":1679822266323,"user_tz":-345,"elapsed":4,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"867a2351-e3e9-4b53-b357-f0ccb61c80b7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Synset('amazon.n.01') :  a large strong and aggressive woman \n","\n","Synset('amazon.n.02') :  (Greek mythology) one of a nation of women warriors of Scythia (who burned off the right breast in order to use a bow and arrow more effectively) \n","\n","Synset('amazon.n.03') :  a major South American river; arises in the Andes and flows eastward into the South Atlantic; the world's 2nd longest river (4000 miles) \n","\n","Synset('amazon.n.04') :  mainly green tropical American parrots \n","\n"]}]},{"cell_type":"code","source":["# Distractors from Wordnet\n","def get_distractors_wordnet(syn,word):\n","    distractors=[]\n","    word= word.lower()\n","    orig_word = word\n","    if len(word.split())>0:\n","        word = word.replace(\" \",\"_\")\n","    hypernym = syn.hypernyms()\n","    if len(hypernym) == 0: \n","        return distractors\n","    for item in hypernym[0].hyponyms():\n","        name = item.lemmas()[0].name()\n","        #print (\"name \",name, \" word\",orig_word)\n","        if name == orig_word:\n","            continue\n","        name = name.replace(\"_\",\" \")\n","        name = \" \".join(w.capitalize() for w in name.split())\n","        if name is not None and name not in distractors:\n","            distractors.append(name)\n","    return distractors\n","\n","synset_to_use = wn.synsets(original_word,'n')[0]\n","distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n","\n","print (\"\\noriginal word: \",original_word.capitalize())\n","print (distractors_calculated)\n","\n","\n","original_word = \"amazon\"\n","synset_to_use = wn.synsets(original_word,'n')[1]\n","distractors_calculated = get_distractors_wordnet(synset_to_use,original_word)\n","\n","print (\"\\noriginal word: \",original_word.capitalize())\n","print (distractors_calculated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbbMso9mz8s7","executionInfo":{"status":"ok","timestamp":1679822269744,"user_tz":-345,"elapsed":782,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"c2d02daf-0cbb-4a88-d217-a9379a1172f1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","original word:  Amazon\n","['B-girl', 'Bachelor Girl', 'Baggage', 'Ball-buster', 'Black Woman', 'Bluestocking', 'Bridesmaid', 'Broad', 'Cat', 'Cinderella', 'Coquette', 'Dame', 'Debutante', 'Divorcee', 'Dominatrix', 'Donna', 'Enchantress', 'Ex-wife', 'Eyeful', 'Geisha', 'Girl', 'Girlfriend', 'Gold Digger', 'Gravida', 'Heroine', 'Inamorata', 'Jezebel', 'Jilt', 'Lady', 'Maenad', 'Matriarch', 'Matron', 'Mestiza', 'Mistress', 'Mother Figure', 'Nanny', 'Nullipara', 'Nymph', 'Nymphet', 'Old Woman', 'Prostitute', 'Shiksa', 'Smasher', 'Sylph', 'Unmarried Woman', 'Vestal', 'Wac', 'Wave', 'White Woman', 'Widow', 'Wife', 'Wonder Woman', 'Yellow Woman']\n","\n","original word:  Amazon\n","['Amazon', 'Gog And Magog', 'Golem', 'Halcyon', 'Hero', 'Houri', 'Mythical Monster', 'Phoenix']\n"]}]},{"cell_type":"code","source":["import os\n","import zipfile\n","\n","bert_wsd_pytorch = \"/content/gdrive/MyDrive/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6.zip\"\n","extract_directory = \"/content/gdrive/My Drive\"\n","\n","extracted_folder = bert_wsd_pytorch.replace(\".zip\",\"\")\n","\n","#  If unzipped folder exists don't unzip again.\n","if not os.path.isdir(extracted_folder):\n","  with zipfile.ZipFile(bert_wsd_pytorch, 'r') as zip_ref:\n","      zip_ref.extractall(extract_directory)\n","else:\n","  print (extracted_folder,\" is extracted already\")"],"metadata":{"id":"_OCO5CDn7qhY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679822277724,"user_tz":-345,"elapsed":656,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"1bba6c4f-e7e0-4f49-bbb3-2f4084810e2f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6  is extracted already\n"]}]},{"cell_type":"code","source":["import torch\n","import math\n","from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer\n","\n","class BertWSD(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.bert = BertModel(config)\n","        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n","\n","        self.ranking_linear = torch.nn.Linear(config.hidden_size, 1)\n","\n","        self.init_weights()\n","\n","\n","# def _forward(args, model, batch):\n","#     batch = tuple(t.to(args.device) for t in batch)\n","#     outputs = model.bert(input_ids=batch[0], attention_mask=batch[1], token_type_ids=batch[2])\n","\n","#     return model.dropout(outputs[1])\n","    \n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_dir = \"/content/gdrive/MyDrive/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6\"\n","\n","\n","model = BertWSD.from_pretrained(model_dir)\n","tokenizer = BertTokenizer.from_pretrained(model_dir)\n","# add new special token\n","if '[TGT]' not in tokenizer.additional_special_tokens:\n","    tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n","    assert '[TGT]' in tokenizer.additional_special_tokens\n","    model.resize_token_embeddings(len(tokenizer))\n","    \n","model.to(DEVICE)\n","model.eval()"],"metadata":{"id":"RkjJ95BF-_ct","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679822393885,"user_tz":-345,"elapsed":35729,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}},"outputId":"719d86fe-c8c5-4af8-bea3-204a1f0d20c9"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/gdrive/MyDrive/bert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6 were not used when initializing BertWSD: ['similarity_linear.weight', 'similarity_loss_factor', 'ranking_loss_factor', 'similarity_linear.bias']\n","- This IS expected if you are initializing BertWSD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertWSD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertWSD(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30523, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (ranking_linear): Linear(in_features=768, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[],"metadata":{"id":"-HgtpszxXJiN","executionInfo":{"status":"ok","timestamp":1679824902468,"user_tz":-345,"elapsed":471,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"urVXig4_XPJF","executionInfo":{"status":"ok","timestamp":1679824902929,"user_tz":-345,"elapsed":2,"user":{"displayName":"Aalok Chhetri","userId":"01407765190030338156"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PMbmscGIXWdm"},"execution_count":null,"outputs":[]}]}